{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAIT_2020_Model_Training_VGG16_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NSr7OxZm6CzVe73hAAhIDyl3l3C-U-RU",
      "authorship_tag": "ABX9TyNSfV0HTQe1LjcW6QYflAAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamtranBKHN/MAIT_2020_LabMemberDectector/blob/origin/MAIT_2020_Model_Training_VGG16_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcc55JuWSolP"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import os\r\n",
        "from keras.applications import VGG16\r\n",
        "from keras.applications import imagenet_utils\r\n",
        "from keras.preprocessing.image import img_to_array\r\n",
        "from keras.preprocessing.image import load_img\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.applications import VGG16\r\n",
        "from keras.layers import Input\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras.layers.core import Dropout\r\n",
        "from keras.layers.core import Flatten\r\n",
        "\r\n",
        "IMG_SIZE = 100"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfu2SV5oUAUx",
        "outputId": "cb8fe69b-1621-42e4-f0ef-98018e393d2b"
      },
      "source": [
        "\r\n",
        "data_dir = '/content/drive/MyDrive/Sample_data_31.12_augmentated'\r\n",
        "print(os.listdir(data_dir))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['THHoang_181888', 'THGiang_192200', 'Thanh_Quang', 'TDPhu_181692', 'Phuong_Anh', 'PGPhong_181691', 'NVMinh_174064', 'NVLinh_181579', 'NTTung_181828', 'NTHLy_181637', 'NTHai_173821', 'NTCong_179626', 'NQThai_192002', 'NQAnh_181325', 'NNHoang_181492', 'Nguyen_Thi_Huong', 'Nguyen_Ba_Hoang', 'NDToan_174273', 'NDQuan_192034', 'NDHuy_161823', 'NBHoang_181486', 'LTHien_173846', 'LDTAnh_181320', 'KTAnh_176917', 'HDTuan_192259', 'DPDuong_181432', 'DLTMy_200426', 'DADung_181421', 'CQDat_181383', 'Bui_Quang_Duy', 'VVHung_192210', 'VCThinh_174239', 'TVThai_181749', 'TVSon_181920', 'Tran_Thi_Thanh_Nhan', 'Tran_Minh_Thuyet', 'TQMinh_181659', 'Toan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXTgV8XU2G_"
      },
      "source": [
        "\r\n",
        "# num_skipped = 0\r\n",
        "\r\n",
        "# for fname in os.listdir(data_dir):\r\n",
        "#   print(\"Processing \" + fname + \" ... \")\r\n",
        "#   personal_img_path = os.path.join(data_dir, fname) \r\n",
        "#   for fname in os.listdir(personal_img_path):\r\n",
        "#     fpath = os.path.join(personal_img_path, fname)\r\n",
        "#     try:\r\n",
        "#         fobj = open(fpath, \"rb\")\r\n",
        "#         is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\r\n",
        "#     finally:\r\n",
        "#         fobj.close()\r\n",
        "\r\n",
        "#     if not is_jfif:\r\n",
        "#         num_skipped += 1\r\n",
        "#         # Delete corrupted image\r\n",
        "#         os.remove(fpath)\r\n",
        "\r\n",
        "# print(\"Deleted %d images\" % num_skipped)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R8Zh_H2U6Lg",
        "outputId": "bf62274e-f98a-4baf-d3db-a888f76c92ca"
      },
      "source": [
        "image_size = (IMG_SIZE, IMG_SIZE)\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "    data_dir,\r\n",
        "    validation_split=0.2,\r\n",
        "    subset=\"training\",\r\n",
        "    seed=1337,\r\n",
        "    image_size=image_size,\r\n",
        "    batch_size=batch_size,\r\n",
        ")\r\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "    data_dir,\r\n",
        "    validation_split=0.2,\r\n",
        "    subset=\"validation\",\r\n",
        "    seed=1337,\r\n",
        "    image_size=image_size,\r\n",
        "    batch_size=batch_size,\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4356 files belonging to 38 classes.\n",
            "Using 3485 files for training.\n",
            "Found 4356 files belonging to 38 classes.\n",
            "Using 871 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwhxdNBtVAyg"
      },
      "source": [
        "data_augmentation = keras.Sequential(\r\n",
        "    [\r\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\r\n",
        "    ]\r\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbkhA3zTVPho"
      },
      "source": [
        "#from keras.utils import to_categorical\r\n",
        "#y = to_categorical(y)\r\n",
        "# Load model VGG 16 của ImageNet dataset, include_top=False để bỏ phần Fully connected lay\r\n",
        "baseModel = VGG16(weights='imagenet', include_top=False, \\\r\n",
        "                  input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\r\n",
        "# Buil layer\r\n",
        "fcHead = baseModel.output\r\n",
        "# Flatten \r\n",
        "fcHead = Flatten()(fcHead)\r\n",
        "# Add FC\r\n",
        "fcHead = Dense(1000, activation='relu')(fcHead)\r\n",
        "fcHead = Dropout(0.5)(fcHead)\r\n",
        "# Output layer with softmax activation\r\n",
        "fcHead = Dense(5, activation='softmax')(fcHead)\r\n",
        "# modle\r\n",
        "model = model = Model(inputs=baseModel.input, outputs=fcHead)\r\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjAQ105sV81G",
        "outputId": "c72cb5a5-33f4-46b2-83d2-593ffe1b1685"
      },
      "source": [
        "epochs = 50\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\r\n",
        "]\r\n",
        "# model.compile(\r\n",
        "#     optimizer=keras.optimizers.Adam(1e-3),\r\n",
        "#     loss=\"binary_crossentropy\",\r\n",
        "#     metrics=[\"accuracy\"],\r\n",
        "# )\r\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", \r\n",
        "                    optimizer = SGD(lr=1e-5, momentum=0.9), \r\n",
        "                    metrics=[\"accuracy\"])\r\n",
        "model.fit(\r\n",
        "    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 24/109 [=====>........................] - ETA: 2:58 - loss: nan - accuracy: 0.0107"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqmk9iYNWIsg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}